

===== FILE: ./copy_content.py =====

#!/usr/bin/env python3

import os

ROOT_DIR = "."                 # start from current directory
OUTPUT_FILE = "combined.txt"   # output file
ENCODING = "utf-8"

with open(OUTPUT_FILE, "w", encoding=ENCODING) as out:
    for root, _, files in os.walk(ROOT_DIR):
        for name in sorted(files):
            # only include Python files
            if not name.endswith(".py"):
                continue

            path = os.path.join(root, name)

            # skip the output file itself (in case it's a .py in future)
            if os.path.abspath(path) == os.path.abspath(OUTPUT_FILE):
                continue

            try:
                with open(path, "r", encoding=ENCODING, errors="ignore") as f:
                    out.write(f"\n\n===== FILE: {path} =====\n\n")
                    out.write(f.read())
            except Exception as e:
                out.write(f"\n\n===== FILE: {path} (ERROR: {e}) =====\n\n")


===== FILE: ./sanity_validate_babappalign.py =====

#!/usr/bin/env python3

import subprocess
import random
from pathlib import Path

AA = "ACDEFGHIKLMNPQRSTVWY"
random.seed(42)

OUT = Path("sanity_runs")
OUT.mkdir(exist_ok=True)


def write_fasta(path, seqs):
    with open(path, "w") as f:
        for k, v in seqs.items():
            f.write(f">{k}\n{v}\n")


def read_fasta(path):
    seqs, cur = {}, None
    for l in open(path):
        l = l.strip()
        if l.startswith(">"):
            cur = l[1:]
            seqs[cur] = ""
        else:
            seqs[cur] += l
    return seqs


def gap_fraction(aln):
    total = sum(len(s) for s in aln.values())
    gaps = sum(s.count("-") for s in aln.values())
    return gaps / total


TESTS = {
    "identity": {
        "a": "MKTAYIAKQRQISFVKSHF",
        "b": "MKTAYIAKQRQISFVKSHF",
    },
    "gap_stress": {
        "a": "MKTAYIAKQRQISFVKSHF",
        "b": "MKTA--AKQRQ--FVKSHF",
        "c": "MKTAYIAK---ISFVKSHF",
    },
    "divergent": {
        f"s{i}": "".join(random.choice(AA) for _ in range(20))
        for i in range(4)
    },
}


print("\n=== BABAPPAlign SANITY VALIDATION vs MAFFT ===\n")

for name, seqs in TESTS.items():
    print(f"▶ TEST: {name}")

    fasta = OUT / f"{name}.fasta"
    mafft_out = OUT / f"{name}.mafft.aln"
    baba_out = OUT / f"{name}.baba.aln"

    write_fasta(fasta, seqs)

    with open(mafft_out, "w") as f:
        subprocess.run(["mafft", "--auto", fasta],
                       stdout=f, stderr=subprocess.DEVNULL, check=True)

    subprocess.run(
        ["babappalign", fasta, "-o", baba_out, "--device", "cpu"],
        
        check=True
    )

    ma = read_fasta(mafft_out)
    ba = read_fasta(baba_out)

    gm = gap_fraction(ma)
    gb = gap_fraction(ba)

    print(f"  Gap fraction MAFFT : {gm:.3f}")
    print(f"  Gap fraction BABAPP: {gb:.3f}")

    if name == "identity" and ma != ba:
        print("  ❌ FAIL: identity mismatch")
    else:
        print("  ✅ PASS")

print("\n=== SANITY VALIDATION COMPLETE ===\n")


===== FILE: ./setup.py =====

from setuptools import setup, find_packages

setup(
    name="babappalign",
    version="1.0.3",
    description="Deep learning–based progressive multiple sequence alignment engine",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    author="Krishnendu Sinha",
    url="https://github.com/sinhakrishnendu/BABAPPAlign",
    packages=find_packages(),
    python_requires=">=3.9",
    install_requires=[
        "numpy",
        "scipy",
        "pandas",
        "biopython",
        "tqdm",
        "torch",
        "fair-esm",
    ],
    include_package_data=True,
    package_data={
        "babappalign": [
            "models/*.pt",
        ]
    },
    entry_points={
        "console_scripts": [
            "babappalign=babappalign.cli:main",
            "babappascore=babappalign.cli:score",
        ]
    },
)


===== FILE: ./babappalign/__init__.py =====

"""
BABAPPAlign: Deep learning–based progressive multiple sequence alignment engine.
"""

__author__ = "Krishnendu Sinha"
__license__ = "MIT"
__version__ = "1.0.3"


===== FILE: ./babappalign/babappalign.py =====

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BABAPPAlign
===========

Embedding-first progressive multiple sequence alignment engine.
"""

from __future__ import annotations

import argparse
import hashlib
import os
import sys
import time
from pathlib import Path
from typing import List, Tuple

import numpy as np


# =========================
# Profile
# =========================

class Profile:
    def __init__(self, ids: List[str], seqs: List[str], idxs=None):
        self.ids = list(ids)
        self.seqs = list(seqs)

        if idxs is None:
            self.idxs = [list(range(len(seqs[0]))) for _ in seqs]
        else:
            self.idxs = idxs

        self.length = len(seqs[0])

    def __len__(self):
        return len(self.seqs)

    def get_column(self, i):
        col = []
        for s, idx in zip(self.seqs, self.idxs):
            if s[i] == "-":
                col.append(("-", None))
            else:
                col.append((s[i], idx[i]))
        return col


# =========================
# Device
# =========================

def resolve_device(user_choice):
    import torch
    if user_choice == "cuda" and torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


# =========================
# Cache
# =========================

def get_cache_dir(subdir: str) -> Path:
    base = Path(os.environ.get("XDG_CACHE_HOME", Path.home() / ".cache"))
    d = base / "babappalign" / subdir
    d.mkdir(parents=True, exist_ok=True)
    return d


def seq_hash(seq: str) -> str:
    return hashlib.sha1(seq.encode()).hexdigest()


# =========================
# FASTA
# =========================

def read_fasta(path: Path) -> Tuple[List[str], List[str]]:
    ids, seqs = [], []
    cur, buf = None, []
    with open(path) as fh:
        for line in fh:
            line = line.strip()
            if not line:
                continue
            if line.startswith(">"):
                if cur is not None:
                    ids.append(cur)
                    seqs.append("".join(buf))
                cur = line[1:].split()[0]
                buf = []
            else:
                buf.append(line)
        if cur is not None:
            ids.append(cur)
            seqs.append("".join(buf))
    return ids, seqs


def write_fasta(ids, seqs, path: Path):
    with open(path, "w") as fh:
        for i, s in zip(ids, seqs):
            fh.write(f">{i}\n{s}\n")


# =========================
# NW profile vs sequence
# =========================

def nw_align_profile_seq(profile, sid, seq, emb_map, model, device, gap_open, gap_extend):
    import torch
    from babappalign.babappascore import batched_score

    m, n = profile.length, len(seq)
    S = np.full((m + 1, n + 1), -1e9)
    T = np.zeros((m + 1, n + 1), dtype=np.int8)

    S[0, 0] = 0.0
    for i in range(1, m + 1):
        S[i, 0] = gap_open + (i - 1) * gap_extend
        T[i, 0] = 1
    for j in range(1, n + 1):
        S[0, j] = gap_open + (j - 1) * gap_extend
        T[0, j] = 2

    for i in range(1, m + 1):
        col = profile.get_column(i - 1)
        for j in range(1, n + 1):
            scores = []
            for (c, idx), pid in zip(col, profile.ids):
                if c == "-":
                    continue
                e1 = emb_map[pid][idx]
                e2 = emb_map[sid][j - 1]
                scores.append(
                    batched_score(
                        e1.unsqueeze(0),
                        e2.unsqueeze(0),
                        model,
                        device
                    )
                )

            match_score = float(torch.mean(torch.stack(scores))) if scores else gap_extend
            match = S[i - 1, j - 1] + match_score
            delete = S[i - 1, j] + gap_extend
            insert = S[i, j - 1] + gap_extend

            best = max(match, delete, insert)
            S[i, j] = best
            T[i, j] = 0 if best == match else (1 if best == delete else 2)

    new_seqs = [""] * len(profile)
    new_idxs = [[] for _ in profile]
    new_seq, new_idx = [], []

    i, j = m, n
    while i > 0 or j > 0:
        t = T[i, j]
        if t == 0:
            for k, (c, idx) in enumerate(profile.get_column(i - 1)):
                new_seqs[k] = c + new_seqs[k]
                new_idxs[k] = [idx] + new_idxs[k]
            new_seq.append(seq[j - 1])
            new_idx.append(j - 1)
            i -= 1
            j -= 1
        elif t == 1:
            for k, (c, idx) in enumerate(profile.get_column(i - 1)):
                new_seqs[k] = c + new_seqs[k]
                new_idxs[k] = [idx] + new_idxs[k]
            new_seq.append("-")
            new_idx.append(None)
            i -= 1
        else:
            for k in range(len(profile)):
                new_seqs[k] = "-" + new_seqs[k]
                new_idxs[k] = [None] + new_idxs[k]
            new_seq.append(seq[j - 1])
            new_idx.append(j - 1)
            j -= 1

    new_seqs.append("".join(reversed(new_seq)))
    new_idxs.append(list(reversed(new_idx)))

    return Profile(profile.ids + [sid], new_seqs, new_idxs)


# =========================
# Progressive
# =========================

def progressive_align(ids, seqs, emb_map, model, device, gap_open, gap_extend):
    prof = Profile([ids[0]], [seqs[0]])
    for sid, seq in zip(ids[1:], seqs[1:]):
        prof = nw_align_profile_seq(
            prof, sid, seq, emb_map, model, device, gap_open, gap_extend
        )
    return prof.ids, prof.seqs


# =========================
# CLI
# =========================

def cli():
    p = argparse.ArgumentParser()
    p.add_argument("sequences")
    p.add_argument("-o", "--output", required=True)
    p.add_argument("--model", default=None)
    p.add_argument("--gap-open", type=float, default=-2.5)
    p.add_argument("--gap-extend", type=float, default=-0.7)
    p.add_argument("--device", choices=["cpu", "cuda"], default=None)
    args = p.parse_args()

    import torch
    from babappalign.babappascore import embed_sequence, safe_load_model

    device = resolve_device(args.device)
    print(f"[info] Using device: {device}")

    ids, seqs = read_fasta(Path(args.sequences))

    emb_cache = get_cache_dir("embeddings")
    emb_map = {}
    for sid, seq in zip(ids, seqs):
        f = emb_cache / f"{seq_hash(seq)}.pt"
        if f.exists():
            emb = torch.load(f, map_location=device)
        else:
            emb = embed_sequence(seq, device)
            torch.save(emb.cpu(), f)
        emb_map[sid] = emb.to(device)

    model = safe_load_model(args.model, device)

    out_ids, out_seqs = progressive_align(
        ids, seqs, emb_map, model, device,
        args.gap_open, args.gap_extend
    )

    write_fasta(out_ids, out_seqs, Path(args.output))


if __name__ == "__main__":
    cli()


===== FILE: ./babappalign/babappascore.py =====

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BABAPPAScore
============

Mandatory learned residue–residue scoring using ESM2 embeddings.

- Single backend: transformers + fair-esm
- Lazy model loading
- CUDA → CPU fallback
- XDG-compliant caching
"""

from __future__ import annotations

import argparse
import os
from pathlib import Path
from typing import Tuple

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel


_ESM2_MODEL = "facebook/esm2_t33_650M_UR50D"
_tokenizer = None
_esm_model = None


# ============================================================
# Cache utilities
# ============================================================

def get_cache_dir(subdir: str) -> Path:
    base = Path(os.environ.get("XDG_CACHE_HOME", Path.home() / ".cache"))
    d = base / "babappalign" / subdir
    d.mkdir(parents=True, exist_ok=True)
    return d


# ============================================================
# ESM embedding
# ============================================================

def load_esm2(device: torch.device):
    global _tokenizer, _esm_model

    if _tokenizer is not None:
        return _tokenizer, _esm_model

    print(f"[info] Loading ESM2 model: {_ESM2_MODEL}")
    _tokenizer = AutoTokenizer.from_pretrained(_ESM2_MODEL)
    _esm_model = AutoModel.from_pretrained(_ESM2_MODEL)
    _esm_model.to(device).eval()

    return _tokenizer, _esm_model


def embed_sequence(seq: str, device: torch.device) -> torch.Tensor:
    tokenizer, model = load_esm2(device)

    inputs = tokenizer(seq, return_tensors="pt", add_special_tokens=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        out = model(**inputs)
        hidden = out.last_hidden_state.squeeze(0)

    # remove CLS and EOS
    return hidden[1:-1].cpu()


# ============================================================
# Learned scorer model
# ============================================================

def safe_load_model(model_path, device):
    import os
    import torch
    from importlib import resources
    from babappalign.pairwise_model import PairwiseScorer

    # -------------------------------------------------
    # Resolve model path
    # -------------------------------------------------
    if model_path is not None:
        path = model_path
    else:
        with resources.path("babappalign.models", "babappascore.pt") as p:
            path = str(p)

    if not os.path.exists(path):
        raise RuntimeError(
            f"BABAPPAScore model not found at {path}. "
            "Learned scoring is mandatory."
        )

    # -------------------------------------------------
    # Reconstruct model architecture
    # -------------------------------------------------
    model = PairwiseScorer()
    state = torch.load(path, map_location=device)

    # Handle both full-model and state_dict cases safely
    if isinstance(state, dict):
        model.load_state_dict(state)
    else:
        model = state

    model.to(device)
    model.eval()
    return model


# ============================================================
# Scoring
# ============================================================

def batched_score(model, A: torch.Tensor, B: torch.Tensor,
                  device: torch.device, batch: int = 4096) -> np.ndarray:

    A = A.to(device)
    B = B.to(device)

    m, _ = A.shape
    n, _ = B.shape

    # vectorized index grid
    ii, jj = torch.meshgrid(
        torch.arange(m), torch.arange(n), indexing="ij"
    )
    ii = ii.flatten()
    jj = jj.flatten()

    S = np.zeros((m, n), dtype=float)

    ptr = 0
    total = len(ii)

    with torch.no_grad():
        while ptr < total:
            end = min(ptr + batch, total)
            ai = A[ii[ptr:end]]
            bj = B[jj[ptr:end]]

            out = model(ai, bj)
            vals = out.detach().cpu().numpy()

            for k in range(end - ptr):
                S[ii[ptr + k], jj[ptr + k]] = float(vals[k])

            ptr = end

    return S


# ============================================================
# CLI
# ============================================================

def cli():
    p = argparse.ArgumentParser(description="BABAPPAScore: deep residue scorer")
    p.add_argument("--seqA", required=True)
    p.add_argument("--seqB", required=True)
    p.add_argument("--model", default=None)
    p.add_argument("--device", choices=["cpu", "cuda"], default=None)
    p.add_argument("--batch", type=int, default=4096)
    p.add_argument("--matrix", default=None)

    args = p.parse_args()

    device = (
        torch.device("cuda")
        if args.device != "cpu" and torch.cuda.is_available()
        else torch.device("cpu")
    )

    def read_fasta(path: Path) -> str:
        return "".join(
            l.strip() for l in open(path) if not l.startswith(">")
        )

    seqA = read_fasta(Path(args.seqA))
    seqB = read_fasta(Path(args.seqB))

    embA = embed_sequence(seqA, device)
    embB = embed_sequence(seqB, device)

    model = safe_load_model(args.model, device)
    M = batched_score(model, embA, embB, device, args.batch)

    print(f"[done] Score matrix shape: {M.shape}")

    if args.matrix:
        np.save(args.matrix, M)


if __name__ == "__main__":
    cli()


===== FILE: ./babappalign/cli.py =====

from babappalign.babappalign import cli as _align_cli
from babappalign.babappascore import cli as _score_cli


def main():
    _align_cli()


def score():
    _score_cli()


===== FILE: ./babappalign/column_model.py =====

# babappalign/column_model.py
import torch
import torch.nn as nn

class ColumnHead(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x).squeeze(1)  # logits


===== FILE: ./babappalign/pairwise_model.py =====

# babappalign/pairwise_model.py
# Publication-grade pairwise scoring model for residue–residue compatibility.
# Symmetric by construction (alignment-safe).

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint


class PairwiseScorer(nn.Module):
    """
    High-quality symmetric scorer with:
    - LayerNorm
    - GELU activation
    - Residual MLP blocks
    - Gradient checkpointing support
    - Xavier initialization

    Inputs:
        e1: [B, D]
        e2: [B, D]
    Output:
        logits: [B]
    """

    def __init__(self, emb_dim=1280, checkpointing=False):
        super().__init__()
        self.checkpointing = checkpointing

        hidden1 = 1024
        hidden2 = 384
        hidden3 = 96

        # Symmetric feature dimension: |e1-e2| and e1*e2
        feat_dim = 2 * emb_dim

        # Input projection
        self.in_proj = nn.Linear(feat_dim, hidden1)

        # Block 1 (residual)
        self.norm1 = nn.LayerNorm(hidden1)
        self.fc1 = nn.Linear(hidden1, hidden1)

        # Block 2
        self.norm2 = nn.LayerNorm(hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)

        # Block 3
        self.norm3 = nn.LayerNorm(hidden2)
        self.fc3 = nn.Linear(hidden2, hidden3)

        # Output head
        self.out = nn.Linear(hidden3, 1)

        self.act = nn.GELU()
        self.drop1 = nn.Dropout(p=0.15)
        self.drop2 = nn.Dropout(p=0.10)

        # -------- Xavier initialization --------
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    # ---- Residual block definitions ----
    def _block1(self, x):
        h = self.norm1(x)
        h = self.fc1(self.drop1(self.act(h)))
        return x + h

    def _block2(self, x):
        h = self.norm2(x)
        h = self.fc2(self.drop2(self.act(h)))
        return h

    def _block3(self, x):
        h = self.norm3(x)
        h = self.fc3(self.act(h))
        return h

    # ------------------- Forward pass -------------------
    def forward(self, e1, e2):
        """
        Symmetric residue–residue scoring.
        Ensures score(e1,e2) == score(e2,e1).
        """

        # ✅ Symmetric feature construction
        diff = torch.abs(e1 - e2)
        prod = e1 * e2
        x = torch.cat([diff, prod], dim=1)

        x = self.in_proj(x)

        if self.checkpointing:
            x = checkpoint.checkpoint(self._block1, x, use_reentrant=False)
            x = checkpoint.checkpoint(self._block2, x, use_reentrant=False)
            x = checkpoint.checkpoint(self._block3, x, use_reentrant=False)
        else:
            x = self._block1(x)
            x = self._block2(x)
            x = self._block3(x)

        x = self.out(x)
        return x.squeeze(1)


===== FILE: ./babappalign/utils.py =====

# babappalign/utils.py
from typing import List, Tuple

def read_fasta(path) -> List[Tuple[str,str]]:
    seqs = []
    with open(path) as f:
        name = None
        seq_lines = []
        for line in f:
            line = line.rstrip()
            if not line:
                continue
            if line.startswith(">"):
                if name is not None:
                    seqs.append((name, "".join(seq_lines)))
                name = line[1:].split()[0]
                seq_lines = []
            else:
                seq_lines.append(line.strip())
        if name is not None:
            seqs.append((name, "".join(seq_lines)))
    return seqs

def write_fasta(seq_list, out_path):
    with open(out_path, "w") as f:
        for name, seq in seq_list:
            f.write(f">{name}\n")
            f.write(seq + "\n")


===== FILE: ./babappalign/models/__init__.py =====

"""
BABAPPAlign: Deep learning–based progressive multiple sequence alignment engine.
"""

__author__ = "Krishnendu Sinha"
__license__ = "MIT"
__version__ = "1.0.3"


===== FILE: ./tests/test_smoke_msa.py =====

"""
Smoke test for BABAPPAlign.

This test is intentionally minimal.
It verifies that the CLI entry point is installed and runnable.

IMPORTANT:
- Must NOT download models
- Must NOT run alignment
- Must work without GPU
- Must work without internet
"""

import subprocess
import sys


def test_babappalign_help():
    """
    Ensure that `babappalign --help` runs successfully.
    """
    result = subprocess.run(
        ["babappalign", "--help"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    assert result.returncode == 0
    assert "BABAPPAlign" in result.stdout or result.stderr
